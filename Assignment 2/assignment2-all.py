# -*- coding: utf-8 -*-
"""Assignment2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q0ViTQcl-eAM4WGt6vKql3a3KKXvNri6

## Mount to Colab runtime
"""

from google.colab import drive
import sys
if('google.colab' in sys.modules):
  print("Google drive detected, mounting...")
  drive.mount('/content/gdrive')
else:
  print("No Google drive found, ignoring...")
print("Done checking")

"""## Installing dependencies"""

# Commented out IPython magic to ensure Python compatibility.
# print("Installing program...")
# if 'google.colab' in sys.modules:
#   !pip install numpy pandas matplotlib seaborn scikit-learn tensorflow keras torch opencv-python labelme statsmodels scipy missingno
# else:
#   %pip install -r studio2.req.txt

"""## Import dependencies"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.signal import find_peaks
from scipy import interpolate

import os
import warnings

"""## Define Config"""

warnings.filterwarnings('ignore')

pd.options.display.max_columns = None
pd.options.display.max_rows = None
pd.options.display.float_format = '{:.7f}'.format

"""## 1. Data Collection

### Define columns, file to read and what class should be used for that file
- Since my student ID ends with 2, thus I will be doing Right Upper Arm (x,y,z) and Left Upper Arm (x,y,z)
- Boning dataset will have a class of '0'
- Slicing dataset will have a class of '1'
- There will also be a 'Frame' column as well
"""

os.chdir("/content/gdrive/MyDrive") if 'google.colab' in sys.modules else None
BASE_PATH = os.getcwd() + "/Colab Notebooks/COS40007/Assignment 2/ampc2" if 'google.colab' in sys.modules else os.getcwd() + "/ampc2"
contents_to_read = {
    'boning': {
        'fName': BASE_PATH + '/Boning.csv',
        'class': 0
    },
    'slicing': {
        'fName': BASE_PATH + '/Slicing.csv',
        'class': 1
    }
}
columns_to_read = [f'Right Upper Arm {k}' for k in ['x', 'y', 'z']] + [f'Left Upper Arm {k}' for k in ['x', 'y', 'z']] + ['Frame']

"""### Read the dataset with chosen columns, append the 'class' feature to the file"""

boning_raw_df = pd.read_csv(contents_to_read['boning']['fName'], usecols=columns_to_read)
boning_df = boning_raw_df.copy()
boning_df['class'] = contents_to_read['boning']['class']
slicing_raw_df = pd.read_csv(contents_to_read['slicing']['fName'], usecols=columns_to_read)
slicing_df = slicing_raw_df.copy()
slicing_df['class'] = contents_to_read['slicing']['class']
print(f"Shape of boning: {boning_df.shape}")
print(f"Shape of slicing: {slicing_df.shape}")

"""### Concat two datasets to be one and save it as combined_data.csv"""

concatenated_df = pd.concat([boning_df, slicing_df], ignore_index=True)
concatenated_df.to_csv(BASE_PATH + "/combined_data.csv", index=False)
concatenated_df.info()

"""## 2. Create Composite Columns

### Read the previously saved dataset
"""

concatenated_raw_df = pd.read_csv(BASE_PATH + "/combined_data.csv")
concatenated_df = concatenated_raw_df.copy()
concatenated_df.info()

concatenated_df.head()

"""### Utils function for calculating Root mean square value, Roll and Pitch"""

def calc_rmsq_for_cols(df: pd.DataFrame, cols: list[str]):
  return np.sqrt(np.mean(df[cols] ** 2, axis=1))

def calc_roll_for_col(df: pd.DataFrame, col: str):
  return 180 * np.arctan2(df[f'{col} y'], np.sqrt(df[f'{col} x'] ** 2 + df[f'{col} z'] ** 2)) / np.pi

def calc_pitch_for_col(df: pd.DataFrame, col: str):
  return 180 * np.arctan2(df[f'{col} x'], np.sqrt(df[f'{col} y'] ** 2 + df[f'{col} z'] ** 2)) / np.pi

"""### Calculate required composite data for Right Hand features


"""

concatenated_df['right_upper_xy_rmsq'] = calc_rmsq_for_cols(concatenated_df, ['Right Upper Arm x', 'Right Upper Arm y'])
concatenated_df['right_upper_yz_rmsq'] = calc_rmsq_for_cols(concatenated_df, ['Right Upper Arm y', 'Right Upper Arm z'])
concatenated_df['right_upper_xz_rmsq'] = calc_rmsq_for_cols(concatenated_df, ['Right Upper Arm x', 'Right Upper Arm z'])
concatenated_df['right_upper_xyz_rmsq'] = calc_rmsq_for_cols(concatenated_df, ['Right Upper Arm x', 'Right Upper Arm y', 'Right Upper Arm z'])

concatenated_df['right_upper_roll'] = calc_roll_for_col(concatenated_df, 'Right Upper Arm')
concatenated_df['right_upper_pitch'] = calc_pitch_for_col(concatenated_df, 'Right Upper Arm')

"""### Calculate required composite data for Left Hand features"""

concatenated_df['left_upper_xy_rmsq'] = calc_rmsq_for_cols(concatenated_df, ['Left Upper Arm x', 'Left Upper Arm y'])
concatenated_df['left_upper_yz_rmsq'] = calc_rmsq_for_cols(concatenated_df, ['Left Upper Arm y', 'Left Upper Arm z'])
concatenated_df['left_upper_xz_rmsq'] = calc_rmsq_for_cols(concatenated_df, ['Left Upper Arm x', 'Left Upper Arm z'])
concatenated_df['left_upper_xyz_rmsq'] = calc_rmsq_for_cols(concatenated_df, ['Left Upper Arm x', 'Left Upper Arm y', 'Left Upper Arm z'])

concatenated_df['left_upper_roll'] = calc_roll_for_col(concatenated_df, 'Left Upper Arm')
concatenated_df['left_upper_pitch'] = calc_pitch_for_col(concatenated_df, 'Left Upper Arm')

concatenated_df.info()

"""### Save it as "composited_data.csv"
"""

concatenated_df.to_csv(BASE_PATH + "/composited_data.csv", index=False)
concatenated_df.head()

"""## 3. Data pre-processing and Feature computation

### Read the previously save "composited_data.csv"
"""

composited_raw_df = pd.read_csv(BASE_PATH + "/composited_data.csv")
composited_df = composited_raw_df.copy()
composited_df.info()

"""### Create statistical features for the 18 columns per minute (1 min = 60fpm)
These includes:
- Mean values
- Standard deviation values
- Min values
- Max values
- Area under the curve (AUC)
- Number of peaks
"""

new_cols = {}
FPM = 60
num_of_min = len(composited_df) // FPM
print(num_of_min)

for column in concatenated_df.columns:
  if column not in ['Frame', 'class']:
    values = {
      'mean': [],
      'max': [],
      'min': [],
      'std': [],
      'auc': [],
      'peak': []
    }
    for i in range(num_of_min):
      start, end = i * FPM, (i + 1) * FPM
      values['mean'].append(np.mean(concatenated_df[column][start:end]))
      values['max'].append(np.max(concatenated_df[column][start:end]))
      values['min'].append(np.min(concatenated_df[column][start:end]))
      values['std'].append(np.std(concatenated_df[column][start:end]))
      values['auc'].append(np.trapz(concatenated_df[column][start:end]))
      peaks, _ = find_peaks(concatenated_df[column][start:end])
      values['peak'].append(len(peaks))
    new_cols[f'{column}_mean'] = values['mean']
    new_cols[f'{column}_max'] = values['max']
    new_cols[f'{column}_min'] = values['min']
    new_cols[f'{column}_std'] = values['std']
    new_cols[f'{column}_auc'] = values['auc']
    new_cols[f'{column}_peak'] = values['peak']

# print(new_cols)

new_features_df = pd.DataFrame(new_cols)
new_features_df['class'] = composited_df['class'][::FPM].reset_index(drop=True)
new_features_df["Minute"] = range(1, num_of_min + 1)

"""### Saved the processed statistical dataset as 'processed_all_data.csv'"""

new_features_df.to_csv(BASE_PATH + "/processed_all_data.csv")
print("Shape: ", new_features_df.shape)
print("Info: ", new_features_df.info())

"""## 4. Training

### Import required dependencies
"""

from sklearn import svm
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.decomposition import PCA
from sklearn.linear_model import SGDClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier

"""### Read the processed data"""

all_data_raw_df = pd.read_csv(BASE_PATH + "/processed_all_data.csv")
all_data_df = all_data_raw_df.copy()
print(all_data_df.head())
print("Info: ", all_data_df.info())

"""### Perform train test split, with 30% of test data"""

X_vals = all_data_df.drop(['class', 'Minute'], axis=1)
Y_vals = all_data_df['class']

X_train, X_test, Y_train, Y_test = train_test_split(X_vals, Y_vals, test_size=0.3, random_state=42)

"""### Predict with Support Vector Machine"""

svc = svm.SVC()
svc.fit(X_train, Y_train)
Y_pred = svc.predict(X_test)
acc = accuracy_score(Y_test, Y_pred)

print(f"Accuracy of the SVM is: {acc * 100:2f}")

"""### 10-fold cross validation mean accuracy of SVM"""

svc = svm.SVC()
cross_val = cross_val_score(svc, X_vals, Y_vals, cv = 10)
print(f"10-fold cross validation mean accuracy score: {cross_val.mean()*100:2f}")

"""### Find the best set of values for the model using GridSearchCV"""

param_grid = {'C': [0.1, 1, 10, 100, 1000],
              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],
              'kernel': ['rbf']}

grid = GridSearchCV(svm.SVC(), param_grid, refit = True, verbose = 3)

# fitting the model for grid search
grid.fit(X_train, Y_train)

print("Best params to fit: ", grid.best_params_)

"""### SVM training and predicting with hyperparameter tuning"""

svc_with_hyp = svm.SVC(C=grid.best_params_['C'], gamma=grid.best_params_['gamma'], kernel=grid.best_params_['kernel'])
svc_with_hyp.fit(X_train, Y_train)

y_pred_with_hyp = svc_with_hyp.predict(X_test)

accuracy_score_with_hyp = accuracy_score(Y_test, y_pred_with_hyp)

print(f"Accuracy of the SVM with hyperparameters tuning: {accuracy_score_with_hyp * 100:2f}")

"""### 10-fold cross validation mean accuracy of SVM with hyperparameter tuning"""

cv_scores_with_hyp = cross_val_score(svc_with_hyp, X_vals, Y_vals, cv = 10)
print(f"10-fold cross validation mean accuracy score with hyperparameter tuning: {cv_scores_with_hyp.mean()*100:2f}")

"""### Select features and split based on the selected features using SelectKBest"""

selector = SelectKBest(f_classif, k=100)
X_selected = selector.fit_transform(X_vals, Y_vals)

X_train, X_test, Y_train, Y_test = train_test_split(X_selected, Y_vals, test_size=0.3, random_state=42)

"""### SVM training and predicting with feature selection + hyperparameter tuning"""

svc_with_hyp.fit(X_train, Y_train)
y_pred_hyp_selected = svc_with_hyp.predict(X_test)
accuracy_score_with_hyp_selected = accuracy_score(Y_test, y_pred_hyp_selected)
print(f"Accuracy with hyperparameter + selected feature:  {accuracy_score_with_hyp_selected * 100:2f}")

"""### 10-fold cross validation mean accuracy of SVM with hyperparameter tuning + features selection"""

cv_scores_with_hyp_selected = cross_val_score(svc_with_hyp, X_selected, Y_vals, cv = 10)
print(f"10-fold cross validation accuracy with hyperparameter + selected feature:  {cv_scores_with_hyp_selected.mean() * 100:2f}")

"""### Perform PCA to reduce dimensionality"""

pca = PCA(n_components=10)
X_pca = pca.fit_transform(X_vals)

X_train, X_test, Y_train, Y_test = train_test_split(X_pca, Y_vals, test_size=0.2, random_state=1)

"""### SVM training and predicting with PCA + hyperparameter tuning"""

svc_with_hyp.fit(X_train, Y_train)
y_pred_hyp_pca = svc_with_hyp.predict(X_test)
accuracy_score_with_hyp_pca = accuracy_score(Y_test, y_pred_hyp_pca)
print(f"Accuracy of the model after PCA: {accuracy_score_with_hyp_pca*100:2f}")

"""### 10-fold cross validation mean accuracy of SVM with hyperparameter tuning + PCA"""

cv_score_with_hyp_pca = cross_val_score(svc_with_hyp, X_pca, Y_vals, cv = 10)
print(f"10-fold cross validation accuracy after PCA: {cv_score_with_hyp_pca.mean() *100:2f}")

"""## Ext 4: SGD, RandomForest and MLPClassifier

### Read the processed data
"""

all_data_raw_df = pd.read_csv(BASE_PATH + "/processed_all_data.csv")
all_data_df = all_data_raw_df.copy()
print(all_data_df.head())
print("Info: ", all_data_df.info())

"""### Perform train test split, with 30% of test data"""

X_data = all_data_df.drop(['class', 'Minute'], axis=1)
Y_data = all_data_df['class']

X_train, X_test, Y_train, Y_test = train_test_split(X_data, Y_data, test_size=0.3, random_state=42)

"""### SGDClassifier training and predicting"""

sgd = SGDClassifier(random_state=42)
sgd.fit(X_train, Y_train)
y_pred_sgd = sgd.predict(X_test)
accuracy_score_sgd = accuracy_score(Y_test, y_pred_sgd)
print(f"Accuracy of the SGD model: {accuracy_score_sgd * 100:2f}")

"""### 10-fold cross validation mean accuracy of SGDClassifier"""

cv_score_sgd = cross_val_score(sgd, X_data, Y_data, cv=10)
cv_score_sgd_mean = cv_score_sgd.mean()
print(f"10-fold cross validation accuracy of the SGD model: {cv_score_sgd_mean * 100:2f}")

"""### RandomForestClassifier training and predicting"""

rf = RandomForestClassifier(random_state=42)
rf.fit(X_train, Y_train)
y_pred_rf = rf.predict(X_test)
accuracy_score_rf = accuracy_score(Y_test, y_pred_rf)
print(f"Accuracy of the Random Forest model: {accuracy_score_rf * 100:2f}")

"""### 10-fold cross validation mean accuracy of RandomForestClassifier"""

cv_score_rf = cross_val_score(rf, X_data, Y_data, cv=10)
cv_score_rf_mean = cv_score_rf.mean()
print(f"10-fold cross validation accuracy of the Random Forest model: {cv_score_rf_mean * 100:2f}")

"""### MLPClassifier training and predicting"""

mlp = MLPClassifier(random_state=42)
mlp.fit(X_train, Y_train)
y_pred_mlp = mlp.predict(X_test)
accuracy_score_mlp = accuracy_score(Y_test, y_pred_mlp)
print(f"Accuracy of the MLP model: {accuracy_score_mlp * 100:2f}")

"""### 10-fold cross validation mean accuracy of MLPClassifier"""

cv_score_mlp = cross_val_score(mlp, X_data, Y_data, cv=10)
cv_score_mlp_mean = cv_score_mlp.mean()
print(f"10-fold cross validation accuracy of the MLP model: {cv_score_mlp_mean * 100:2f}")